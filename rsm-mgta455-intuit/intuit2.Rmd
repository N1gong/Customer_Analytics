---
title: Intuit Quickbooks Upgrade
output: html_document
---

* Team-lead GitLab id:
* Group number:
* Group name:
* Team member names:

```{r r_setup, include = FALSE}
## initial settings
knitr::opts_chunk$set(
  comment = NA,
  echo = TRUE,
  error = TRUE,
  cache = FALSE,
  message = FALSE,
  dpi = 144,
  warning = FALSE
)

## width to use when printing tables etc.
options(
  width = 250,
  scipen = 100,
  max.print = 5000,
  stringsAsFactors = FALSE
)

## load radiant packages if needed
if (!exists("r_environment")) library(radiant)
```

<style>
.table {
  width: auto;
}
ul, ol {
  padding-left: 18px;
}
pre, code, pre code {
  overflow: auto;
  white-space: pre;
  word-wrap: normal;
  background-color: #ffffff;
}
</style>

## Setup

Please complete this R-markdown document with your group by answering the questions in `intuit-quickbooks.pdf` on Dropbox (week6/readings/). Create an HTML file with all your results and comments and push both the Rmarkdown and HTML file to GitLab when your team is done. All results MUST be reproducible (i.e., the TA and I must be able to recreate the HTML from the Rmarkdown file without changes or errors). This means that you should NOT use any R-packages that are not part of the rsm-msba-spark docker container.

This is the first group assignment for MGTA 455 and you will be using git and GitLab. If two people edit the same file at the same time you could get what is called a "merge conflict". git will not decide for you who's change to accept so the team-lead will have to determine which edits to use. To avoid merge conflicts, **always** click "pull" in Rstudio before you start working on a files. Then, when you are done, save and commit your changes, and then push them to GitLab. Make this a habit!

If multiple people are going to work on the assignment at the same time I recommend you work on different files. You can use `source` to include R-code in your Rmarkdown document or include other R(markdown) documents into the main assignment file. 

Group work-flow tips as discussed during ICT in Summer II are shown below:

* Pull, edit, save, stage, commit, and push
* Schedule who does what and when
* Try to avoid working simultaneously on the same file 
* If you are going to work simultaneously, do it in different files, e.g., 
    - assignment1_john.R, assignment1_susan.R, assignment1_wei.R 
    - assignment_1a.R, assignment_1b.R, assignment_1c.R
* Use the `source` command to bring different pieces of code together into an Rmarkdown document or into an R-code file
* Alternatively, use _child_ in Rmarkdown to include a part of a report
* For (very) big projects use 'branches' to avoid conflicts (and stay on your branch)

A graphical depiction of the group work-flow is shown below:

![](images/git-group-workflow.png)

Additional resource on the use of git are linked below:

* http://happygitwithr.com
* http://r-pkgs.had.co.nz/git.html
* http://stackoverflow.com/questions/tagged/git or just a google search
* https://try.github.io
* https://www.manning.com/books/git-in-practice
* https://github.com/GitInPractice/GitInPractice#readme  


```{r include=FALSE}
library(tidyverse)
intuit75k <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/intuit75k.rds"))
```  

## Question answers  


## Part I: Exploratory Data Analysis:  
&nbsp;
&nbsp;  

### First, let's see the correlations among all the variables  
&nbsp;  

```{r echo=FALSE}
### correlations
result <- correlation(
  intuit75k, 
  vars = c(
    "zip_bins", "bizflag", "numords", "dollars", "last", "sincepurch", 
    "version1", "owntaxprod", "upgraded"
  )
)
# summary(result)
plot(result, nrobs = 1000)
```  
&nbsp;
&nbsp;  

### Next, we want to explore how response rate varies in terms of each feature we're interested in.  

```{r echo=FALSE}
## zip_bins
intuit75k %>%
  group_by(zip_bins) %>%
  mutate(zip_bins_resp = mean(res1 == "Yes")) %>%
  distinct(zip_bins, zip_bins_resp) %>%
  arrange(zip_bins) %>%
  ggplot(aes(x=zip_bins, y=zip_bins_resp, fill = zip_bins)) + geom_bar(stat = 'identity')

## sex
intuit75k %>%
  group_by(sex) %>%
  mutate(sex_resp = mean(res1 == "Yes")) %>%
  distinct(sex, sex_resp) %>%
  arrange(sex) %>%
  ggplot(aes(x=sex, y=sex_resp, fill = sex)) + geom_bar(stat = 'identity')

## bizflag
intuit75k %>%
  group_by(bizflag) %>%
  mutate(bizflag_resp = mean(res1 == "Yes")) %>%
  distinct(bizflag, bizflag_resp) %>%
  arrange(bizflag) %>%
  ggplot(aes(x=bizflag, y=bizflag_resp, fill = bizflag)) + geom_bar(stat = 'identity')
    
## numords
intuit75k %>%
  group_by(numords) %>%
  mutate(numords_resp = mean(res1 == "Yes")) %>%
  distinct(numords, numords_resp) %>%
  arrange(numords) %>%
  ggplot(aes(x=numords, y=numords_resp, fill = numords)) + geom_bar(stat = 'identity')

## dollars
intuit75k %>%
  group_by(dollars) %>%
  mutate(dollars_resp = mean(res1 == "Yes")) %>%
  distinct(dollars, dollars_resp) %>%
  arrange(dollars) %>%
  ggplot(aes(x=dollars, y=dollars_resp, fill = dollars)) + geom_bar(stat = 'identity')

## last
intuit75k %>%
  group_by(last) %>%
  mutate(last_resp = mean(res1 == "Yes")) %>%
  distinct(last, last_resp) %>%
  arrange(last) %>%
  ggplot(aes(x=last, y=last_resp, fill = last)) + geom_bar(stat = 'identity')

## sincepurch
intuit75k %>%
  group_by(sincepurch) %>%
  mutate(sincepurch_resp = mean(res1 == "Yes")) %>%
  distinct(sincepurch, sincepurch_resp) %>%
  arrange(sincepurch) %>%
  ggplot(aes(x=sincepurch, y=sincepurch_resp, fill = sincepurch)) + geom_bar(stat = 'identity')

## version1
intuit75k %>%
  group_by(version1) %>%
  mutate(version1_resp = mean(res1 == "Yes")) %>%
  distinct(version1, version1_resp) %>%
  arrange(version1) %>%
  ggplot(aes(x=version1, y=version1_resp, fill = version1)) + geom_bar(stat = 'identity')

## owntaxprod
intuit75k %>%
  group_by(owntaxprod) %>%
  mutate(owntaxprod_resp = mean(res1 == "Yes")) %>%
  distinct(owntaxprod, owntaxprod_resp) %>%
  arrange(owntaxprod) %>%
  ggplot(aes(x=owntaxprod, y=owntaxprod_resp, fill = owntaxprod)) + geom_bar(stat = 'identity')

## upgraded
intuit75k %>%
  group_by(upgraded) %>%
  mutate(upgraded_resp = mean(res1 == "Yes")) %>%
  distinct(upgraded, upgraded_resp) %>%
  arrange(upgraded) %>%
  ggplot(aes(x=upgraded, y=upgraded_resp, fill = upgraded)) + geom_bar(stat = 'identity')

```  
&nbsp;
&nbsp;  

**We can reach the following conclusions based on the EDA output:**  
&nbsp;  

1. The customers in zip_bins = 1 have obviously higher response rate than other zip bin customers.  

2. The variables "sex" and "bizflag" have no significant effect on "res1".  

3. Based on the "Recency, Frequency and Monetary" framework, we are more interested in how recent a customer made the purchase other than how early he become made the first order; we hence pick "last" over "sincepurch" to be used in the model.  

4. Also based on the "Recency, Frequency and Monetary" framework, we are more concerned about how frequent a customer put orders with the company than how much money he has spent; besides, there's a strong relationship between the two features as usually the more frequent customers buy the more money customers spent in total. Therefore, we think it is only necessary to use one of them - "numords" - in our model.  

> Based on the finding that customers in zip_bines = 1 have obviously higher response rate than other zip bin customers, we think it worth trying to give more weight to customers in this zip bin when predicting purchase probability. A new variable "zip_one" is created to enable the model to do so.  

&nbsp;  

```{r echo=FALSE}
intuit75k <- intuit75k %>% mutate(zip_fac = factor(zip_bins),
                                  zip_one = ifelse(zip_fac == 1, TRUE, FALSE))
head(intuit75k[, c(1,2,3,16)])
```  
&nbsp;  

> We then further investigated to see why bin1 has such high response rate. Breaking-down the actual zip code, we try to uncover the truth by exploring response rate by state(represented by first 3-digit of zip). Finally, we find that Virginia whose zip-code start with "008" has 1891 responses with response rate of 0.398, much higher than the other states. Thus, we create another new variable "VI".  

&nbsp;

```{r echo=FALSE}
intuit75k <- intuit75k %>% mutate(state= substr(zip, 1, 3),
                                  VI = ifelse(substr(zip, 1, 3) == '008', TRUE, FALSE))

head(intuit75k[, c(1,2,3,18)])
```  

&nbsp;
&nbsp;
&nbsp;
&nbsp;  

***  


## PART II Initial Model Development  

&nbsp;
&nbsp;  

```{r include=FALSE}
mail_cost <- 1.41
sales_margin <- 60
break_even_resp <- mail_cost / sales_margin
```  
  
**Based one the cost and margin given, we use the break-even rate of `r format_nr(break_even_resp, dec = 3)` as the cut-off to lable whether Intuit should mail to in the second round or not.**  
&nbsp;  
&nbsp;  

```{r include=FALSE}
perf_eval <- function(df) {
  
  cus <- nrow(df)
  perc_mail <- mean(df$mailto_wave2)
  dat <- filter(df, mailto_wave2 == TRUE)
  resp <- mean(dat$res1 == 'Yes')
  act_resp <- mean(df$res1 == 'Yes') 

  nummail <- sum(df$mailto_wave2)
  mailcost <- nummail * mail_cost

  exp_buyer <- sum(dat$res1 == 'Yes')
  act_buyer <- sum(df$res1 == 'Yes')

  exp_margin <- exp_buyer*sales_margin
  act_margin <- sum(df$res1 == 'Yes') * sales_margin

  profit <- exp_buyer *sales_margin - mailcost
  ROME <- profit / mailcost
  
  AUC <- auc(as.numeric(df$mailto_wave2), df$res1, 'Yes')
  
  prnt <- paste0("Based on our analysis, the number of customers Intuit should mail is ", format_nr(nummail, dec = 0), " that is ", format_nr(perc_mail, perc = TRUE), " of the customers.</br> The response rate for the selected customers is predicted to be ", format_nr(resp, perc = TRUE), ", or, ", format_nr(exp_buyer, dec = 0), " buyers; while the actual response rate is ", format_nr(act_resp, perc = TRUE), ", or, ", format_nr(act_buyer, dec=0), ".</br> The predicted margin is ", format_nr(exp_margin, "$", dec = 2), "; while actual margin is ", format_nr(act_margin, "$", dec=2),  ".</br> The expected profit is ", format_nr(profit, "$", dec = 0), ". The messaging cost is estimated to be ", format_nr(mailcost, "$", dec = 0), " with a ROME of ", format_nr(ROME, perc = FALSE), ".")

  data.frame(nummail, perc_mail, resp, exp_buyer, exp_margin, profit, mailcost, ROME, AUC, prnt)

}
```  
&nbsp;  
&nbsp;


## Sequential RFM model  
&nbsp;  
&nbsp;

> We indexing the rfm-id to every cutsomer in the dataset, and split the data into training and validation set. Using break-even as cut-off on the training set, we filtered out the profitable rfm-id to mail to in the validation set.  

&nbsp;  
&nbsp;  

```{r include=FALSE}
intuit75k <- intuit75k %>%
  mutate(rec = xtile(last, 5)) %>%
  group_by(rec) %>%
  mutate(freq =xtile(numords, 5, rev = TRUE)) %>%
  group_by(rec, freq) %>%
  mutate(mon = xtile(dollars, 5, rev = TRUE)) %>%
  mutate(rfm_sq = paste0(rec, freq, mon)) %>%
  ungroup()

train <- intuit75k %>%
  filter(training == 1) %>%
  group_by(rfm_sq) %>%
  mutate(rfm_resp = mean(res1 == 'Yes'),
         mailto_wave2 = mean(res1 == 'Yes') > break_even_resp) %>%
  ungroup()
```  

**partial RFM-id**
```{r echo=FALSE}
rfm_id <- train$rfm_sq[train$mailto_wave2 == TRUE]
rfm_id[1:10]
```  


```{r include=FALSE}
# Predict on Val
val <- intuit75k %>%
  filter(training == 0)

val$mailto_wave2 <- ifelse(val$rfm_sq %in% rfm_id, TRUE, FALSE)

val <- val %>% 
  group_by(rfm_sq) %>% 
  mutate(rfm_resp = mean(res1 == 'Yes')) %>% 
  ungroup()

```  


```{r include=FALSE}
perf_rfm_train <- perf_eval(train)
perf_rfm_val <- perf_eval(val)
```  
&nbsp;  
&nbsp;


### Evaulate the performance of sequential RFM model  

&nbsp;  
&nbsp;  

**Training**
```{r echo=FALSE, results="asis"}
cat(perf_rfm_train$prnt)
```  

&nbsp;  
&nbsp;  

**Validation**
```{r echo=FALSE, results="asis"}
cat(perf_rfm_val$prnt)
```  
&nbsp;  
&nbsp;
&nbsp;  
&nbsp;  


## Logistic Regression  
&nbsp;  
&nbsp;  

**Uncertain about whether to choose "zip_one", which weighs more on all customers in zip_bin 1, or "VI", which weighs more on only customer in the state of Virginia area, we decided to build 2 models for each variable then pick the one that contributes more profit in the validation set.**   

&nbsp;  

**For both models, we re-estimate them 100 times, each time with a different bootstrap sample of the data; then calculate the 5th percentile of the predictions to use as the lower bound on the estimated probability.**  
&nbsp;  
&nbsp;  
&nbsp;  

### Logistic Regression A  

#### with "zip_one", "numords", "last", "version1", "owntaxprod", "upgraded" as predictors.  
&nbsp;  
&nbsp;
     
```{r echo=FALSE}
res_A <- logistic(
  train, 
  rvar = "res1", 
  evar = c("zip_one", "numords", "last", "version1", "owntaxprod", "upgraded"),
  lev = "Yes", 
)
summary(res_A)
```  

     
```{r include=FALSE}
set.seed(1234)

log_tr_predA <- data.frame(id = train$id)
log_val_predA <- data.frame(id = val$id)

for (i in 1:100) {
  
  t_logA <- sample_n(train, 52500, replace = TRUE)
 
  res_logA <- logistic(
  t_logA, 
  rvar = "res1", 
  evar = c("zip_one", "numords", "last", "version1", "owntaxprod", "upgraded"),      # del dollars
  lev = "Yes", 
  # check = "standardize"
)
  
  pred_logA <- predict(res_logA, pred_data = train)
  pred_logvA <- predict(res_logA, pred_data = val)
  
  log_tr_predA <- store(log_tr_predA, pred_logA, name = paste0("pred_logit", i))
  log_val_predA <- store(log_val_predA, pred_logvA, name = paste0("pred_val_logit", i))
  
}
```  
&nbsp;  
&nbsp;  

#### bootstrap sample predcition on the training set
```{r echo=FALSE}
head(log_tr_predA)
```  
&nbsp;  
&nbsp;  

#### bootstrap sample predcition on the validation set  
```{r echo=FALSE}
head(log_val_predA)
```
&nbsp;  
&nbsp;  

#### get 5th percentile lower-bound probability  
&nbsp;  
&nbsp;  

**Trainging predcition**
```{r echo=FALSE}
train_lbA <- data.frame(id = log_tr_predA$id, prob_log_lbA = apply(log_tr_predA[, -1], 1, quantile, probs = 0.05))
train <- train %>% 
  left_join(train_lbA, by = 'id')
head(train[, c(1,24)])
```  
&nbsp;  
&nbsp;  

**Validation prediction**  
```{r echo=FALSE}
val_lbA <- data.frame(id = log_val_predA$id, prob_log_lbA = apply(log_val_predA[, -1], 1, quantile, probs = 0.05))
val <- val %>% 
  left_join(val_lbA, by = 'id')
head(val[, c(1,24)])
```  
&nbsp;  
&nbsp;
&nbsp;  


### Logistic Regression B  

#### with "VI", "numords", "last", "version1", "owntaxprod", "upgraded" as predictors.  
&nbsp;  
&nbsp;  

```{r echo=FALSE}
res_B <- logistic(
  train, 
  rvar = "res1", 
  evar = c("VI", "numords", "last", "version1", "owntaxprod", "upgraded"),
  lev = "Yes", 
)
summary(res_B)
```  


```{r include=FALSE}
set.seed(1234)

log_tr_predB <- data.frame(id = train$id)
log_val_predB <- data.frame(id = val$id)

for (i in 1:100) {
  
  t_logB <- sample_n(train, 52500, replace = TRUE)
 
  res_logB <- logistic(
  t_logB, 
  rvar = "res1", 
  evar = c("VI", "numords", "last", "version1", "owntaxprod", "upgraded"), 
  lev = "Yes", 
  # check = "standardize"
)
  
  pred_logB <- predict(res_logB, pred_data = train)
  pred_logvB <- predict(res_logB, pred_data = val)
  
  log_tr_predB <- store(log_tr_predB, pred_logB, name = paste0("pred_logit", i))
  log_val_predB <- store(log_val_predB, pred_logvB, name = paste0("pred_val_logit", i))
  
}
```  
&nbsp;  

#### bootstrap sample predcition on the training set
```{r echo=FALSE}
head(log_tr_predB)
```  
&nbsp;  
&nbsp;  

#### bootstrap sample prediction on the validation set  
```{r echo=FALSE}
head(log_val_predB)
```  
&nbsp;  
&nbsp;

#### get 5th percentile lower-bound probability  
&nbsp;  
&nbsp;  

**Trainging predcition**
```{r echo=FALSE}
train_lbB <- data.frame(id = log_tr_predB$id, prob_log_lbB = apply(log_tr_predB[, -1], 1, quantile, probs = 0.05))
train <- train %>% 
  left_join(train_lbB, by = 'id')
head(train[, c(1, 25)])
```  
&nbsp;  
&nbsp;  

**Validation prediction**
```{r echo=FALSE}
val_lbB <- data.frame(id = log_val_predB$id, prob_log_lbB = apply(log_val_predB[, -1], 1, quantile, probs = 0.05))
val <- val %>% 
  left_join(val_lbB, by = 'id')
head(val[, c(1, 25)])
```  
&nbsp;  
&nbsp;
&nbsp;  


### Compare Logistic model A performance with Logistic model B  

```{r include=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = prob_log_lbA > break_even_resp)

val <- val %>% 
  mutate(mailto_wave2 = prob_log_lbA > break_even_resp)

perf_log_trainA <- perf_eval(train)
perf_log_valA <- perf_eval(val)
```  
&nbsp;
&nbsp;

#### Model performance using "zip_one"  
&nbsp;
&nbsp;  

**Training**
```{r echo=FALSE, results="asis"}
perf_log_trainA$prnt
```  
&nbsp;  

**Validation**
```{r echo=FALSE, results="asis"}
perf_log_valA$prnt
```  
&nbsp;
&nbsp;  

#### Model performance using "VI"  
&nbsp;
&nbsp;  

**Training**
```{r include=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = prob_log_lbB > break_even_resp)

val <- val %>% 
  mutate(mailto_wave2 = prob_log_lbB > break_even_resp)

perf_log_trainB <- perf_eval(train)
perf_log_valB <- perf_eval(val)
```  


**Training**
```{r echo = FALSE, results="asis"}
perf_log_trainB$prnt
```  
&nbsp;  

**Validation**
```{r echo=FALSE, results="asis"}
perf_log_valB$prnt
```  

  

```{r include=FALSE}
perf_2logs <- data.frame(
  name = c("log.train.zip_one", "log.train.VI", "log.val.zip_one", "log.val.VI"),
  Profit = c(perf_log_trainA$profit, perf_log_trainB$profit, perf_log_valA$profit, perf_log_valB$profit),
  ROME = c(perf_log_trainA$ROME, perf_log_trainB$ROME, perf_log_valA$ROME, perf_log_valB$ROME)
)
```  
&nbsp;
&nbsp;
  
#### Profit  
&nbsp;  

```{r echo=FALSE}
visualize(
  perf_2logs,
  xvar = "name",
  yvar = "Profit",
  type = "bar",
  labs = list(title = "compare profit", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(Profit, dec = 2)), vjust = 2)
```  
&nbsp;
&nbsp;  

#### ROME  
&nbsp;  

```{r echo=FALSE}
visualize(
  perf_2logs,
  xvar = "name",
  yvar = "ROME",
  type = "bar",
  labs = list(title = "compare ROME", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(ROME, dec = 2)), vjust = 2)
```  
&nbsp;
&nbsp;
&nbsp;
&nbsp;  

## Naive Bayes  
  
#### Based on the result from the logistic regression model, we'll use "VI", "dollars", "last", "version1", "owntaxprod", "upgraded" in the Naive Bayes model.
&nbsp;  

**Created with Laplace = 1 to avoid situation where the predictor predict 0 on unseen event**  

```{r echo=FALSE}
nb_res <- nb(
  train, 
  rvar = "res1", 
  evar = c("VI", "numords", "dollars", "last", "version1", "owntaxprod", "upgraded"), 
  laplace = 1
)
summary(nb_res)
```  
&nbsp;
&nbsp;  

**Training Prediction**
&nbsp;
```{r echo=FALSE}
pred <- predict(nb_res, pred_data = train)
print(pred, n = 10)
train <- store(train, pred, name = "pred_nb")
```  

```{r include=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = pred_nb > break_even_resp)
```  

**Validation Prediction**  
&nbsp;
```{r echo=FALSE}
pred <- predict(nb_res, pred_data = val)
print(pred, n = 10)
val <- store(val, pred, name = "pred_nb")
```

```{r include=FALSE}
val <- val %>% 
  mutate(mailto_wave2 = pred_nb > break_even_resp)
```

```{r include=FALSE}
perf_nb_train <- perf_eval(train)
perf_nb_val <- perf_eval(val)
```  
&nbsp;
&nbsp;  


### Evaulate the performance of Naive Bayes Model  
&nbsp;  
&nbsp;  

**Training**
```{r echo=FALSE, results="asis"}
perf_nb_train$prnt
```  
&nbsp;
&nbsp;  

**Validation**
```{r echo=FALSE, results="asis"}
perf_nb_val$prnt
```  
&nbsp;
&nbsp;
&nbsp;
&nbsp;  


## Nerual Network Model  
&nbsp;  

> Due to the distinct ability of Nerual Network model to capture the complexity of the relationship and interactions between features and response variable, we decide to first feed the model with all the important features to see if the importance of variables deduced by Nerual Network model conforms to our conclusions made before.  

&nbsp;  
&nbsp;  

```{r echo=FALSE, fig.height=5.38, fig.width=7, dpi=144}
result <- nn(
  intuit75k, 
  rvar = "res1", 
  evar = c(
    "numords", "dollars", "last", "sincepurch", "version1", 
    "owntaxprod", "upgraded", "zip_one", "VI"
  ), 
  lev = "Yes", 
  size = 2, 
  seed = 1234, 
  data_filter = "train <- training == 1"
)
summary(result, prn = TRUE)
plot(result, plots = "olden", custom = FALSE)
```  
&nbsp;  

> The Olden plot shows that "dollars", "last", "sincepurch" and "zip_one" are relatively less important factor to consider when predicting purchase probabilities. This is aligned with our findings so we'll build the Nerual Network model using the same variables as in logistics. We'll use the same method to get the 5th lower-bound prediction as the purchase probability to lable customers as in the Logistic Regression models.  

&nbsp;
&nbsp;  

```{r include=FALSE}
nn_pred_tr <- read_rds("NNTr.rds")
nn_pred_val <- read_rds("NNVal.rds")
```  

#### bootstrap sample prediction on the training set
```{r echo=FALSE}
head(nn_pred_tr)
```  
&nbsp;  

#### bootstrap sample predcition on the validation set  

```{r echo=FALSE}
head(nn_pred_val)
```  
&nbsp;  

#### Get 5th percentile lower-bound probability  
&nbsp;
&nbsp;  

**Trainging prediction**  

```{r echo=FALSE}
train_lbnn1 <- data.frame(id = nn_pred_tr$id, prob_nn_lb1 = apply(nn_pred_tr[, -1], 1, quantile, probs = 0.05))
train <- train %>% 
  left_join(train_lbnn1, by = 'id')
head(train[, c(1,27)])
```  
&nbsp;  
**Validation Prediction**  

```{r echo=FALSE}
val_lbnn1 <- data.frame(id = nn_pred_val$id, prob_nn_lb1 = apply(nn_pred_val[, -1], 1, quantile, probs = 0.05))
val <- val %>% 
  left_join(val_lbnn1, by = 'id')
head(val[, c(1, 27)])
```  


```{r include=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = prob_nn_lb1 > break_even_resp)
```


```{r include=FALSE}
val <- val %>% 
  mutate(mailto_wave2 = prob_nn_lb1 > break_even_resp)
```


```{r include=FALSE}
perf_nn_train <- perf_eval(train)
perf_nn_val <- perf_eval(val)
```  
&nbsp;
&nbsp;  

### Evaulate the performance of Neural Networks model  

**Trainging predcition**  
```{r echo=FALSE, results="asis"}
perf_nn_train$prnt
```  
&nbsp;  

**Validation predcition** 
```{r echo=FALSE, results="asis"}
perf_nn_val$prnt
```  


## PART III Performance Evaluation for All Models  
&nbsp;
&nbsp;  

**Now that we've already built four models, it's time to review their performance altogether. We'll first compare profits and ROME for both training and validation data, then visualize the lift and gains under different models to evaluate the efficiency; lastly, we'll compare the models' AUC score and construct the confursion matrix.**  
&nbsp;  

```{r include=FALSE}
perf_all_trn <- data.frame(
  name = c("RFM. sq", "logit. lb", "naive.Bayes", "Neural Network"),
  Profit = c(perf_rfm_train$profit, perf_log_trainB$profit, perf_nb_train$profit, perf_nn_train$profit),
  ROME = c(perf_rfm_train$ROME, perf_log_trainB$ROME, perf_nb_train$ROME, perf_nn_train$ROME)
)
Dp <- (max(perf_all_trn$Profit) - perf_all_trn$Profit[2]) / perf_all_trn$Profit[2]
```  


```{r include=FALSE}
perf_all_val <- data.frame(
  name = c("RFM. sq", "logit. lb", "naive.Bayes", "Neural Network"),
  Profit = c(perf_rfm_val$profit, perf_log_valB$profit, perf_nb_val$profit, perf_nn_val$profit),
  ROME = c(perf_rfm_val$ROME, perf_log_valB$ROME, perf_nb_val$ROME, perf_nn_val$ROME)
)

VDp <- (max(perf_all_val$Profit) - perf_all_val$Profit[4]) / perf_all_val$Profit[4]
```  
&nbsp;  

### Profit - training set  
&nbsp;  

The predicted training profit is highest under NN model, leading the second highest profit which is under Logistics model by `r format_nr(Dp, perc=TRUE)`. However, we think the outcome might because of NN model's strong learning ability in the training data and there could potentially be an 'overfit' compared to logistics model. The validation outcome can speak more on this matter.  

&nbsp;

```{r echo=FALSE}
## Train
visualize(
  perf_all_trn,
  xvar = "name",
  yvar = "Profit",
  type = "bar",
  labs = list(title = "traing profit", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(Profit, dec = 2)), vjust = 2)
```  
&nbsp;  

### Profit - validation set  
&nbsp;

Looking at the validation prediction result, NN is still performing well. But this time, profit under NN is surpassed by logistic model by `r format_nr(VDp, perc=TRUE)`. Based on such result, we think logistic model can generalize the data better hence the best model to use as our final model in targeting clients.  

&nbsp;  

```{r echo=FALSE}
## Val
visualize(
  perf_all_val,
  xvar = "name",
  yvar = "Profit",
  type = "bar",
  labs = list(title = "validation profit", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(Profit, dec = 2)), vjust = 2)

```  
&nbsp;  
&nbsp;  

### ROME - Training set  
&nbsp;  

Under both training set and validation set, Naive Nayes model outperforms all the other models in return on marketing expenditure; however, considering its performance in predicting profits, we cannot prefer this model over logistics in term of targeting customers.  

&nbsp;  

```{r echo=FALSE}
## Train
visualize(
  perf_all_trn,
  xvar = "name",
  yvar = "ROME",
  type = "bar",
  labs = list(title = "traing ROME", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(ROME, dec = 2)), vjust = 2)
```  
&nbsp;  

### ROME - Validation set
&nbsp;  

```{r echo=FALSE}
## Val
visualize(
  perf_all_val,
  xvar = "name",
  yvar = "ROME",
  type = "bar",
  labs = list(title = "validation ROME", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(ROME, dec = 2)), vjust = 2)
```  


```{r include=FALSE}
new_intuit <- rbind(train, val)
```  
&nbsp;
&nbsp;  

### Lift and Gains  

From the lift chart, we see that the machine learning models are much more efficient predicting and targeting purchase than non-machine-learning model, Sequential RFM. The 3 machine-learning models almost make no differences in the lift and gains charts, but we can still see that the Logistic model and NN model are able to gain more responses than other models when targeting the same percentage of customers. It suggests that using these two models can help Intuit generate more profit with less budget exhausted.  

&nbsp;  

```{r echo=FALSE, fig.height=21.54, fig.width=7.54, dpi=144}
eval_class <- evalbin(
  new_intuit, 
  pred = c("rfm_resp", "prob_log_lbB", "pred_nb", "prob_nn_lb1"), 
  rvar = "res1", 
  lev = "Yes", 
  cost = 1.41, 
  margin = 60, 
  train = "Both", 
  data_filter = "training == 1"
)
# summary(eval_class, prn = FALSE)
plot(
  eval_class, 
  plots = c("lift", "gains"), 
  custom = FALSE
)
```  

&nbsp;
&nbsp;  

### Confusion matrix  

&nbsp;  

Investigating the confusion matrix, we can figure out how logistic model surpassed NN when predicted on the validation data. As NN model may learn the pattern in the training data too well, it inherited a more strict standard to judge a customer's purchase probability; therefore, it would give many customers lower probabilities compared to logistic models, who instead generalized the data and judge the customers more liberal. Due to this reason, NN has lower FP and higher TNR while LR has higher FP but lower TNR. In our case, the mail cost is extremely low compared to the high margin, so it costs Intuit virtually nothing to send out a bit more mail but costs a lot if missed a potential buyer.  

&nbsp;  

Based on the confusion matrix, we reconsolidated our confidence that LR model should be the best one to use in the final prediction.  

&nbsp;
```{r echo=FALSE, fig.height=12.92, fig.width=7, dpi=144}
eval_conf <- confusion(
  new_intuit, 
  pred = c("rfm_resp", "prob_log_lbB", "pred_nb", "prob_nn_lb1"), 
  rvar = "res1", 
  lev = "Yes", 
  cost = 1.41, 
  margin = 60, 
  train = "Both", 
  data_filter = "training == 1"
)
summary(eval_conf)
plot(eval_conf, custom = TRUE)
```  
&nbsp;
&nbsp;
&nbsp;
&nbsp;  

***  

## PART VI - Re-evaluate Model Performance with Lower Projected Response Rate  

&nbsp;
&nbsp;
```{r include=FALSE}
train <- train %>% 
  select(.,-mailto_wave2)
val <- val %>% 
  select(.,-mailto_wave2)
new_cutoff <- break_even_resp * 2
```  


> As we know that in Wave2, customers will be less likely to response than in Wave1. To make sure we have picked the best model, we will re-evaluate the 4 models using a higher cut-off to lable whether they'll response or not. The new cut-off is double the break-even response rate used in Part I~III to reflect the 50% reduction of response rate. **The new cutoff is `r format_nr(new_cutoff, dec=2)`.**  

&nbsp;
&nbsp;


### Sequential RFM model  

&nbsp;
&nbsp;

**revised profitable rfm-id**
```{r echo=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = mean(train$res1 == 'Yes') > new_cutoff)

rfm_id <- train$rfm_sq[train$mailto_wave2 == TRUE]
rfm_id[1:10]
```  


```{r include=FALSE}
val$mailto_wave2 <- ifelse(val$rfm_sq %in% rfm_id, TRUE, FALSE)

perf_rfm_train <- perf_eval(train)
perf_rfm_val <- perf_eval(val)
```  
&nbsp;
&nbsp;

### Evaulate RFM performance  
&nbsp;  

**Training**
```{r echo=FALSE, results="asis"}
perf_rfm_train$prnt
```  
&nbsp;  

**Validation**
```{r echo=FALSE, results="asis"}
perf_rfm_val$prnt
```  
&nbsp;
&nbsp;
&nbsp;  

### Logistics Regression  

**To be prudent, we re-do the process in previous part to see if "VI" is indeed a better predictor than "zip_one".**  
&nbsp;
&nbsp;  

### Performance using 'zip_one'  
&nbsp;
&nbsp;  

```{r include=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = prob_log_lbA > new_cutoff)

val <- val %>% 
  mutate(mailto_wave2 = prob_log_lbA > new_cutoff)
```  
&nbsp;
&nbsp;  

```{r include=FALSE}
perf_log_trainA <- perf_eval(train)
perf_log_valA <- perf_eval(val)
```  
&nbsp;
&nbsp;  

**Training**  

```{r echo = FALSE, results="asis"}
perf_log_trainA$prnt
```  
&nbsp;

**Validation**  

```{r echo=FALSE, results="asis"}
perf_log_valA$prnt
```  
&nbsp;
&nbsp;  


### Performance using 'VI'  
&nbsp;
&nbsp;  

```{r include=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = prob_log_lbB > new_cutoff)

val <- val %>% 
  mutate(mailto_wave2 = prob_log_lbB > new_cutoff)
```


```{r include=FALSE}
perf_log_trainB <- perf_eval(train)
perf_log_valB <- perf_eval(val)
```  

**Training**  

```{r echo = FALSE, results="asis"}
perf_log_trainB$prnt
```  
&nbsp;  

**Validation**  

```{r echo=FALSE, results="asis"}
perf_log_valB$prnt
```  
&nbsp;
&nbsp;  

### Compare Profit and ROME  

&nbsp;  

> From the profit comparison chart we can see that "VI" is a better predictor in the logistic model to provide higher profit than "zip_one", even when response rate decrease. This time, even ROME is much higher using "VI" as the predictor to target customers.  

&nbsp;

```{r include=FALSE}
perf_2logs <- data.frame(
  name = c("log.train.zip_one", "log.train.VI", "log.val.zip_one", "log.val.VI"),
  Profit = c(perf_log_trainA$profit, perf_log_trainB$profit, perf_log_valA$profit, perf_log_valB$profit),
  ROME = c(perf_log_trainA$ROME, perf_log_trainB$ROME, perf_log_valA$ROME, perf_log_valB$ROME)
)
```  
  
#### Profit  

```{r echo=FALSE}
visualize(
  perf_2logs,
  xvar = "name",
  yvar = "Profit",
  type = "bar",
  labs = list(title = "compare profit", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(Profit, dec = 2)), vjust = 2)
```  
&nbsp;
&nbsp;  

#### ROME  

```{r echo=FALSE}
visualize(
  perf_2logs,
  xvar = "name",
  yvar = "ROME",
  type = "bar",
  labs = list(title = "compare ROME", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(ROME, dec = 2)), vjust = 2)
```  
&nbsp;
&nbsp;
  

### Naive Bayes  

```{r include=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = pred_nb > new_cutoff)

val <- val %>% 
  mutate(mailto_wave2 = pred_nb > new_cutoff)
```  
&nbsp;
&nbsp;  

### Performance  

```{r include=FALSE}
perf_nb_train <- perf_eval(train)
perf_nb_val <- perf_eval(val)
```  
&nbsp;
&nbsp;  

**Training**
```{r echo=FALSE, results="asis"}
perf_nb_train$prnt
```  
&nbsp;  

**Validation**
```{r echo=FALSE, results="asis"}
perf_nb_val$prnt
```  
&nbsp;
&nbsp;
&nbsp;  

### Nerual Networks Model  
&nbsp;
&nbsp;  


```{r include=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = prob_nn_lb1 > new_cutoff)
```

```{r include=FALSE}
val <- val %>% 
  mutate(mailto_wave2 = prob_nn_lb1 > new_cutoff)
```

#### Evaulate the performance of Neural Networks model  

```{r include=FALSE}
perf_nn_train <- perf_eval(train)
perf_nn_val <- perf_eval(val)
```  
&nbsp;
&nbsp;

**Training**
```{r echo=FALSE, results="asis"}
perf_nn_train$prnt
```  
&nbsp;  

**validation** 
```{r echo=FALSE, results="asis"}
perf_nn_val$prnt
```  
&nbsp;
&nbsp;
&nbsp;
&nbsp;  

### Performance re-Evaluation for All Models  

**Let's review again the performance altogether. We'll first compare profits and ROME for both training and validation data, then visualize the lift and gains under different models to evaluate the efficiency; lastly, we'll compare the models' AUC score and construct the confursion matrix.**  
&nbsp;
&nbsp;  

```{r include=FALSE}
perf_all_trn <- data.frame(
  name = c("RFM. sq", "logit. lb", "naive.Bayes", "Neural Network"),
  Profit = c(perf_rfm_train$profit, perf_log_trainB$profit, perf_nb_train$profit, perf_nn_train$profit),
  ROME = c(perf_rfm_train$ROME, perf_log_trainB$ROME, perf_nb_train$ROME, perf_nn_train$ROME)
)
Dp <- (max(perf_all_trn$Profit) - perf_all_trn$Profit[4]) / perf_all_trn$Profit[4]
```  


```{r include=FALSE}
perf_all_val <- data.frame(
  name = c("RFM. sq", "logit. lb", "naive.Bayes", "Neural Network"),
  Profit = c(perf_rfm_val$profit, perf_log_valB$profit, perf_nb_val$profit, perf_nn_val$profit),
  ROME = c(perf_rfm_val$ROME, perf_log_valB$ROME, perf_nb_val$ROME, perf_nn_val$ROME)
)

VDp <- (max(perf_all_val$Profit) - perf_all_val$Profit[4]) / perf_all_val$Profit[4]
```  


#### Profit - training set  
&nbsp;
&nbsp;  

The predicted highest training profit is now under logistic model, leading the second highest profit which is under NN model by `r format_nr(Dp, perc=TRUE)`. The predicted highest validation profit is also under logistic model.  

```{r echo=FALSE}
## Train
visualize(
  perf_all_trn,
  xvar = "name",
  yvar = "Profit",
  type = "bar",
  labs = list(title = "traing profit", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(Profit, dec = 2)), vjust = 2)
```  
&nbsp;
&nbsp;  

#### Profit - validation set  

```{r echo=FALSE}
## Val
visualize(
  perf_all_val,
  xvar = "name",
  yvar = "Profit",
  type = "bar",
  labs = list(title = "validation profit", x = ""),
  custom = TRUE
) +
  geom_text(aes(label = format_nr(Profit, dec = 2)), vjust = 2)

```  


```{r include=FALSE}
new_intuit2 <- rbind(train, val)
```  

#### Lift and Gains  
&nbsp;  

From the lift chart, we're now able to see a distinct advantage of logistic and NN model over other models. Sequential RFM. Also in the gains charts, the Logistic model and NN model are able to gain more purchases than other models when targeting the same proportion of customers. It suggests that using these two models can help Intuit generate more profit with less budget exhausted.  

```{r echo=FALSE, fig.height=21.54, fig.width=7.54, dpi=144}
eval_class2 <- evalbin(
  new_intuit2, 
  pred = c("rfm_resp", "prob_log_lbB", "pred_nb", "prob_nn_lb1"), 
  rvar = "res1", 
  lev = "Yes", 
  cost = 1.41, 
  margin = 60, 
  train = "Both", 
  data_filter = "training == 1"
)
# summary(eval_class, prn = FALSE)
plot(
  eval_class2, 
  plots = c("lift", "gains"), 
  custom = FALSE
)
```  
&nbsp;
&nbsp;  


#### Confusion matrix  
&nbsp;  

Investigating the confusion matrix at the new cut-off, we can still see the similar prediction pattern of how logistic model surpassed NN when predicted on the validation data. 
&nbsp;  

Based on the confusion matrix, we reconsolidated our confidence that LR model is the best one to use in the final prediction.  
&nbsp;  

```{r echo=FALSE, fig.height=12.92, fig.width=7, dpi=144}
eval_conf2 <- confusion(
  new_intuit2, 
  pred = c("rfm_resp", "prob_log_lbB", "pred_nb", "prob_nn_lb1"), 
  rvar = "res1", 
  lev = "Yes", 
  cost = 1.41, 
  margin = 60, 
  train = "Both", 
  data_filter = "training == 1"
)
summary(eval_conf2)
plot(eval_conf, custom = TRUE)
```  
&nbsp;
&nbsp;
&nbsp;  

***  

## PART V - Customer Selection  
&nbsp;
&nbsp;  

> Based on the exploratory data analysis and the modeling process, we decide to use the prediction result of Logistic Regression B from PART VI as the guideline to target customers.  


```{r include=FALSE}
train <- train %>% 
  mutate(mailto_wave2 = prob_log_lbB > new_cutoff)

id_list <- val %>% 
  mutate(mailto_wave2 = prob_log_lbB > new_cutoff) %>% 
  select(id, mailto_wave2)

saveRDS(id_list, file = "Nawen_Xinyue_Zichen_Zheng_Team12.rds", ascii = FALSE, version = NULL,
       compress = TRUE, refhook = NULL)
```  
***  









