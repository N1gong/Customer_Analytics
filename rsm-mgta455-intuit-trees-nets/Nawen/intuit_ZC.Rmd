
```{r}
library(tidyverse)
library(radiant)
library(randomForest)
library(xgboost)
library(zipcode)
library(ranger)
library(caret)
library(rpart)
```  


```{r}
intuit <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/intuit75k.rds"))
data("zipcode")
```


### Clean up Zipcode  

```{r}
intuit <- intuit %>% 
  left_join(zipcode, by = 'zip') %>% 
  select(., -c( 9,17, 18))

intuit <- intuit %>% 
  mutate(VI = ifelse(substr(zip, 1, 3) == '008', TRUE, FALSE),
         zip801 = ifelse(zip == '00801', TRUE, FALSE),
         zip804 = ifelse(zip == '00804', TRUE, FALSE),
         numords_version1 = numords * version1,
         last_version1 = last * version1)
intuit <- intuit %>% 
  group_by(state) %>% 
  mutate(state_resp = mean(res1=='Yes')) %>% 
  ungroup()

intuit <- intuit %>% 
  mutate(state_bin = xtile(state_resp, 20, rev = TRUE))

intuit$state_bin <- factor(intuit$state_bin)
intuit$zip_bins <- factor(intuit$zip_bins)

train <- intuit %>% 
  filter(training == 1) %>% 
  select(res1, zip_bins, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804, numords_version1, last_version1)

val <- intuit %>% 
  filter(training == 0) %>% 
  select(res1, zip_bins, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804, numords_version1, last_version1)

```  


Function for Calculating Profit
```{r}
break_even <- 1.41/60

perf_profit <- function(df) {
  
  dat <- filter(df, mailto_wave2 == TRUE)
  
  nummail <- sum(df$mailto_wave2 == TRUE)
  mailcost <- nummail * 1.41
  
  profit <- sum(dat$res1 == 'Yes')  * 60 - mailcost
  profit

}
```  

## Scaled Profit
```{r include=FALSE}
## For Scaled Profit
perf_eval <- function(df) {
  total_cus <- 801821
  resp_wave1 <- 38487
  unresp <- total_cus - resp_wave1

  dat <- filter(df, mailto_wave2 == TRUE)
  resp <- mean(dat$res1 == 'Yes') * 0.5
  perc_nummail <- sum(df$mailto_wave2) / nrow(df)
  scaled_mail <- perc_nummail * unresp
  mailcost <- scaled_mail * 1.41

  scaled_profit <- unresp*perc_nummail* resp * 60 - mailcost
  scaled_profit
}
  
```

shuffle training data  

```{r}
set.seed(1234)
train <- train[sample(nrow(train)), ]
```

### Create Grid Search Hyperparameter for Random Forest  

```{r}
# Establish a list of possible values for minsplit and maxdepth
mtry <- seq(3, 7, 2)
node_size  = seq(80, 120, 20)
sampe_size = c(.55, .632, .70, .80)

# Create a data frame containing all combinations 
RF_grid <- expand.grid(mtry = mtry, sample.fraction = sampe_size, min.node.size = node_size)

# Check out the grid
head(RF_grid)
RF_grid[28,]

# Print the number of grid combinations
nrow(RF_grid)
```  


### Train RF

```{r}
# Number of potential models in the grid
num_models <- nrow(RF_grid)

# Create an empty list to store models
RF_models <- list()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:num_models) {

    mtry <- RF_grid$mtry[i]
    sample.fraction <- RF_grid$sample.fraction[i]
    min.node.size <- RF_grid$min.node.size[i]
    
    RF_models[[i]] <- ranger(res1 ~ .- numords_version1-last_version1 , probability = TRUE,
              data = train, num.trees = 501,
              mtry = mtry, respect.unordered.factors = TRUE, seed = 1234,
              sample.fraction = sample.fraction,
              min.node.size = min.node.size)
}

saveRDS(RF_models, "RF_models453.rds")

```  


### Val Test

```{r}
num_models <- length(RF_models)

profit_rf_values <- c()

for (i in 1:num_models){
  
  mod <- RF_models[[i]]

  pred_val_RF <- predict(mod, val)
  
 # pred_rfr <- pred_rfr$predictions[, 1]

  profit_rf_values[i] <- perf_eval(tibble(res1 = val$res1, 
                                          mailto_wave2 = pred_val_RF$predictions[, 1] > break_even*2))
  
}

max(profit_rf_values)
which.max(profit_rf_values)   ## 453757.3
best_RF <- RF_models[[28]]     ##mtry=3, node size: 120 
```  

### Search for better try
```{r}
# Establish a list of possible values for minsplit and maxdepth
mtry <- seq(3, 9, 2)
node_size  = seq(60, 120, 20)
sampe_size = c(.55, .632, .70, .80, .90)
ntree <- seq(501, 507, 2)

# Create a data frame containing all combinations 
tree_grid <- expand.grid(mtry = mtry, node_size = node_size, sampe_size = sampe_size, ntree = ntree)

# Check out the grid
head(tree_grid)

# Print the number of grid combinations
nrow(tree_grid)


# Number of potential models in the grid
num_models <- nrow(tree_grid)

# Create an empty list to store models
RF2_models <- list()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:num_models) {

    num.trees <- tree_grid$ntree[i]
    mtry <- tree_grid$mtry[i]
    sample.fraction <- tree_grid$sampe_size[i]
    min.node.size <- tree_grid$node_size[i]
    
    RF2_models[[i]] <- ranger(res1 ~ .- numords_version1-last_version1 , probability = TRUE,
              data = train, num.trees = num.trees,
              mtry = mtry, respect.unordered.factors = TRUE, seed = 1234,
              sample.fraction = sample.fraction,
              min.node.size = min.node.size)
}


num_models <- length(RF2_models)

profit_rf2_values <- c()

for (i in 1:num_models){
  
  mod <- RF2_models[[i]]

  pred_val2_RF <- predict(mod, val)

  profit_rf2_values[i] <- perf_eval(tibble(res1 = val$res1, 
                                          mailto_wave2 = pred_val2_RF$predictions[, 1] > break_even*2))
  
}
#which.max(profit_rf2_values)
#RF2_models[[2]]
```  


### Predict with best model
```{r}
pred_rgr <- predict(best_RF, val)

perf_eval(tibble(res1 = val$res1, mailto_wave2 =pred_rgr$predictions[, 1] > break_even*2))
```




### NN 5 fold cv  


NN_models <- read_rds("NN_models.rds")

trainNN <- intuit %>% 
  filter(training == 1) %>% 
  select(res1, state_bin, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804)
valNN <- intuit %>% 
  filter(training == 0) %>% 
  select(res1, state_bin, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804)



```{r}
# str(train_dat)
nrow(train)/5

## Create 5 folds
f1 <- seq(1,10500)
f2 <- seq(10500+1 ,10500*2)
f3 <- seq(10500*2+1 ,10500*3)
f4 <- seq(10500*3+1 ,10500*4 )
f5 <- seq(10500*4+1, 10500*5)

## Create grid search

size <- seq(1, 5, 1)
decay <- c(0.1, 0.5, 0.1)
# decayx <- c(0.1, 0.5, 1, 1.15, 1.5)
test <- list(f1, f2, f3, f4, f5)

nn_grid <- expand.grid(size = size, decay = decay, indx = test)
# nn_gridx <- expand.grid(size = size, decay = decayx, indx = test)
# Print the number of grid combinations
nrow(nn_grid)

#set.seed(1234)
num_NN_model <- nrow(nn_grid)
NN_models <- list()
set.seed(1234)
for (i in 1:num_NN_model) {
  
  size <- nn_grid$size[i]
  decay <- nn_grid$decay[i]
  test_indx <- nn_grid$indx[[i]]
  
  NN_models[[i]] <- nn(
    train[-(test_indx), ], 
    rvar = "res1", 
    evar = c('zip_bins', 'numords', 'dollars', 'last', 'version1', 'owntaxprod', 'upgraded', 'zip801', 'zip804'),
    lev = "Yes", 
    size = size,
    decay = decay,
    seed = 1234 
    )
  
}

saveRDS(NN_models, "NN_models1.rds")

```  


```{r}

num_models <- length(NN_models)

profit_nn_values <- c()

for (i in 1:num_models){
 
  test_indx <- nn_grid$indx[[i]]
  mod <- NN_models[[i]]

  pred_tr_nn <- predict(mod, pred_data = train[test_indx, ])
  #print(pred_tr_nn)
  
  profit_nn_values[i] <- perf_eval(tibble(res1 = train[test_indx,]$res1, 
                                            mailto_wave2 = pred_tr_nn$Prediction> break_even*2))
  
}

which.max(profit_nn_values) ## 17  ## 32now

best_NN <- NN_models[[37]]

#best_NN_mod <- NN_models[[17]]  ## 3 / 17028.03/ decay 0.01/ 4


```  


### Try on Val-set
```{r}
pred_val_nn <- predict(best_NN, pred_data = val)

NN_Val_prof <- perf_eval(tibble(res1=val$res1, mailto_wave2 = pred_val_nn$Prediction > break_even*2))

NN <- tibble(res1=val$res1, mailto_wave2 = pred_val_nn$Prediction > break_even*2)
# 28-6-1 decay=1
sum(NN$mailto_wave2 == FALSE & NN$res1 == 'Yes')

```  


### Train with more Variables
```{r}
trainNN <- intuit %>% 
  filter(training==1) %>% 
  select(res1, zip_bins, sex, bizflag, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804)
valNN <-  intuit %>% 
  filter(training==0) %>% 
  select(res1, zip_bins, sex, bizflag, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804)
# str(train_dat)
nrow(trainNN)/5

## Create 5 folds
f1 <- seq(1,10500)
f2 <- seq(10500+1 ,10500*2)
f3 <- seq(10500*2+1 ,10500*3)
f4 <- seq(10500*3+1 ,10500*4 )
f5 <- seq(10500*4+1, 10500*5)

## Create grid search

size2 <- seq(2, 6, 1)
decay2 <- c(0.1, 0.5, 0.1)
# decayx <- c(0.1, 0.5, 1, 1.15, 1.5)
test <- list(f1, f2, f3, f4, f5)

nn_grid2 <- expand.grid(size = size2, decay = decay2, indx = test)
# nn_gridx <- expand.grid(size = size, decay = decayx, indx = test)
# Print the number of grid combinations
nrow(nn_grid2)

#set.seed(1234)
num_NN_model2 <- nrow(nn_grid2)
NN_models2 <- list()
set.seed(1234)
for (i in 1:num_NN_model2) {
  
  size2 <- nn_grid2$size[i]
  decay2 <- nn_grid2$decay[i]
  test_indx <- nn_grid2$indx[[i]]
  
  NN_models2[[i]] <- nn(
    trainNN[-(test_indx), ], 
    rvar = "res1", 
    evar = c('zip_bins',  'sex', 'bizflag', 'numords', 'dollars', 'last', 'version1', 'owntaxprod', 'upgraded', 'zip801', 'zip804'),
    lev = "Yes", 
    size = size2,
    decay = decay2,
    seed = 1234 
    )
  
}

saveRDS(NN_models, "NN_models2.rds")
```  



### No substaintial improvement
```{r}

num_models2 <- length(NN_models2)

profit_nn_values2 <- c()

for (i in 1:num_models2){
 
  test_indx <- nn_grid2$indx[[i]]
  mod <- NN_models2[[i]]

  pred_tr_nn <- predict(mod, pred_data = trainNN[test_indx, ])
  #print(pred_tr_nn)
  
  profit_nn_values2[i] <- perf_eval(tibble(res1 = trainNN[test_indx,]$res1, 
                                            mailto_wave2 = pred_tr_nn$Prediction> break_even*2))
}

which.max(profit_nn_values2) ## 17  ## 32now   ## 69 w/ more var

best_NN2 <- NN_models2[[69]]

best_NN_mod <- NN_models[[17]]  ## 3 / 17028.03/ decay 0.01/ 4
```  
 

### XGBoosting

#### Using all training set

```{r}
# N <- nrow(train)*0.8
train_xgb <- train[, -c(11,12)]
test_xgb <- val[, -c(11,12)]

#using one hot encoding 
labels <- ifelse(train_xgb$res1 == 'Yes', 1, 0)
ts_label <- ifelse(test_xgb$res1 == 'Yes', 1, 0)

new_tr <- model.matrix(~.+0,data = train_xgb[,-1]) 
new_ts <- model.matrix(~.+0,data = test_xgb[,-1])

#preparing matrix 
dtrain <- xgb.DMatrix(data = new_tr,label = labels) 
dtest <- xgb.DMatrix(data = new_ts,label = ts_label)
```  


## tune using CV
```{r}
scale_pos <- sum(train$res1 == 'No') / sum(train$res1 == 'Yes') * (nrow(val)/nrow(train))

#paramsX<- list(booster = "gbtree", objective = "binary:logistic", eta=0.3, gamma=5, max_depth=9, min_child_weight = 5, subsample=0.55, colsample_bytree=0.85)
set.seed(1234)
paramsX<- list(booster = "gbtree", objective = "binary:logistic", eta = 0.3, gamma=7, subsample=0.5, colsample_bytree=0.8, max_delta_step=10, min_child_weight=10)

xgbcv <- xgb.cv( params = paramsX, data = dtrain, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 20, maximize = F)

set.seed(1234)
xtr <- xgb.train (data = dtrain, nrounds = 25, params = paramsX)

xpred <- predict(xtr, dtest)

perf_eval(tibble(res1=val$res1, mailto_wave2 = xpred*0.5> break_even))
perf_profit(tibble(res1=val$res1, mailto_wave2 = xpred > break_even*2, xgb_prob = xpred))

xgb <- tibble(res1=val$res1, mailto_wave2 = xpred > break_even*2, xgb_prob = xpred)
35643.27 / sum(xgb$mailto_wave2)*1.41 
sum(xgb$mailto_wave2 == TRUE)

sum(xgb$mailto_wave2 == FALSE & xgb$res1 == 'Yes')
# disable_default_eval_metric = 1, feval = pos_rate 
```


XGBClassifier(
    learning_rate =0.007,
    n_estimators=1000,
    max_depth = 3,
    min_child_weight = 5,
    gamma=0.4,
    subsample=0.55,
    colsample_bytree=0.85,
    reg_alpha=0.005,
    objective= 'binary:logistic',
    nthread=4,
    scale_pos_weight=1,
    seed=27
)

### Create hyper-grid for XGB model

XGB_models <- read_rds('XGB_model.rds')

```{r}
eta <- c(0.003, 0.005, 0.007)
max_depth <- c(6, 8, 10)
colsample_bytree <- seq(0.65, 0.75, 0.85)
gamma <- c(8, 10, 12)
max_delta_step <- c(8, 10, 12)
min_child_weight <- c(8, 10, 12)


XGB_grid <- expand.grid(max_depth = max_depth, gamma=gamma, max_delta_step = max_delta_step, min_child_weight = min_child_weight)

num_XGB_models <- nrow(XGB_grid)
XGB_models <- list()

set.seed(1234)
for (i in 1:num_XGB_models){
  eta <- XGB_grid$eta[i]
  max_depth <- XGB_grid$max_depth[i]
  gamma <- XGB_grid$gamma[i]
  colsample_bytree <- XGB_grid$colsample_bytree[i]
  
  params <- list(booster = "gbtree", objective = "binary:logistic", eta=eta, gamma=gamma, max_depth=max_depth, subsample=0.55, colsample_bytree=0.8, scale_pos_weight=9)
  
  XGB_models[[i]] <- xgb.train (params = params, data = dtrain, nrounds = 38)
}
saveRDS(XGB_models, 'XGB_model.rds')
```


```{r}
#XGB_models <- readRDS('XGB_model1.rds')
num_models <- length(XGB_models)

profit_xgb_values <- c()

for (i in 1:num_models){
 
  mod <- XGB_models[[i]]

  pred_val_xgb <- predict(mod, dtest)
  profit_xgb_values[i] <- perf_eval(tibble(res1=val$res1, mailto_wave2 = pred_val_xgb > break_even*2))
  
}

# which.max(profit_xgb_values)
which.max(profit_xgb_values)
best_xgb <- XGB_models[[1]]
pred_val_xgb <- predict(best_xgb, dtest)
perf_eval(tibble(res1=val$res1, mailto_wave2 = pred_val_xgb > break_even*2))
xgb_df <- tibble(res1=val$res1, mailto_wave2 = pred_val_xgb > break_even*2)

```  



## Ensembel  

```{r}
set.seed(1234)
val_pred <- cbind(rf_pred = pred_rgr$predictions[, 1], pred_nn = pred_val_nn$Prediction, pred_xgb = xpred)
mean_resp <- rowMeans(val_pred)
##  46008.68
ensembel_profit <- perf_eval(tibble(res1=val$res1, mailto_wave2 = mean_resp > break_even*2))
```




