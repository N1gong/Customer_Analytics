---
title: Intuit Trees and Nets
output: html_document
---

* Team-lead GitLab id: rsm-n1gong
* Group number: 12
* Group name: Team 12
* Team member names: Nawen Gong, Zichen Huang, Xinyue Tao, Zheng Shan

```{r r_setup, include = FALSE}
## initial settings
knitr::opts_chunk$set(
  comment = NA,
  echo = TRUE,
  error = TRUE,
  cache = FALSE,
  message = FALSE,
  dpi = 144,
  warning = FALSE
)

## width to use when printing tables etc.
options(
  width = 250,
  scipen = 100,
  max.print = 5000,
  stringsAsFactors = FALSE
)

## load radiant packages if needed
if (!exists("r_environment")) library(radiant)
```

<style>
.table {
  width: auto;
}
ul, ol {
  padding-left: 18px;
}
pre, code, pre code {
  overflow: auto;
  white-space: pre;
  word-wrap: normal;
  background-color: #ffffff;
}
</style>

## Setup

Please complete this R-markdown document with your group by working through the guidelines in `intuit-trees-nets.pdf` on Dropbox (week7/readings/). Create an HTML file with all your results and comments and push both the Rmarkdown and HTML file to GitLab when your team is done. All results MUST be reproducible (i.e., the TA and I must be able to recreate the HTML from the Rmarkdown file without changes or errors). Also, make sure the code is well commented and formatted (e.g., use styler through Addins > Style active file in Rstudio).

This is the second group assignment for MGTA 455 and you will be using git and GitLab. If two people edit the same file at the same time you could get what is called a "merge conflict". git will not decide for you who's change to accept so the team-lead will have to determine which edits to use. To avoid merge conflicts, always click "pull" in Rstudio before you start working on file. Then, when you are done, commit your changes, and push them to GitLab. Make this a habit!

If multiple people are going to work on the assignment at the same time I recommend you work on different files. You can use `source` to include R-code in your Rmarkdown document or include other R(markdown) documents into the main assignment file. 

Group work-flow tips are listed from ICT in summer are shown below:

* Pull, edit, save, stage, commit, and push
* Schedule who does what and when
* Try to avoid working simultaneously on the same file 
* If you are going to work simultaneously, do it in different files, e.g., 
    - assignment1_john.R, assignment1_susan.R, assignment1_wei.R 
    - assignment1a.R, assignment1b.R, assignment1c.R
* Use the 'source' command to bring different pieces of code together in an Rmarkdown document or in an R-code file
* Alternatively, use _child_ in Rmarkdown to include a part of a report
* For (very) big projects use 'branches' to avoid conflicts (and stay on your branch)

A graphical depiction of the group work-flow is shown below:

![](images/git-group-workflow.png)

Additional resource on the use of git are linked below:

* http://happygitwithr.com
* http://r-pkgs.had.co.nz/git.html
* http://stackoverflow.com/questions/tagged/git or just a google search
* https://try.github.io
* https://www.manning.com/books/git-in-practice
* https://github.com/GitInPractice/GitInPractice#readme

```{r}
library(tidyverse)
library(radiant)
library(randomForest)
library(xgboost)
library(zipcode)
library(ranger)
library(caret)
library(rpart)
```  


```{r}
## loading the data. Note that data must be loaded from the data/
## in the rstudio project directory
intuit <- readr::read_rds(file.path(radiant.data::find_dropbox(), "MGTA455-2019/data/intuit75k.rds"))
data("zipcode")
```

### Clean up Zipcode  

```{r}
data("zipcode")
intuit <- intuit %>%
  left_join(zipcode, by = "zip") %>%
  select(., -c(9, 17, 18))

intuit <- intuit %>%
  mutate(
    VI = ifelse(substr(zip, 1, 3) == "008", TRUE, FALSE),
    zip801 = ifelse(zip == "00801", TRUE, FALSE),
    zip804 = ifelse(zip == "00804", TRUE, FALSE),
    numords_version1 = numords * version1,
    last_version1 = last * version1
  )

intuit$zip_bins <- factor(intuit$zip_bins)

train <- intuit %>%
  filter(training == 1) %>%
  select(res1, zip_bins, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804, numords_version1, last_version1)

val <- intuit %>%
  filter(training == 0) %>%
  select(res1, zip_bins, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804, numords_version1, last_version1)

id <- intuit %>%
  filter(training == 0) %>%
  select(id)
```  


Function for Calculating Profit
```{r}
breakeven_response <- 1.41 / 60

perf_profit <- function(df) {
  dat <- filter(df, mailto == TRUE)

  nummail <- sum(df$mailto == TRUE)
  mailcost <- nummail * 1.41

  profit <- sum(dat$res1 == "Yes") * 60 - mailcost
  profit
}
```  


Function for Calculating Scaled Profit
```{r include=FALSE}
## For Scaled Profit
perf_eval <- function(df) {
  total_cus <- 801821
  resp_wave1 <- 38487
  unresp <- total_cus - resp_wave1

  dat <- filter(df, mailto_wave2 == TRUE)
  resp <- mean(dat$res1 == "Yes") * 0.5
  perc_nummail <- sum(df$mailto_wave2) / nrow(df)
  scaled_mail <- perc_nummail * unresp
  mailcost <- scaled_mail * 1.41

  scaled_profit <- unresp * perc_nummail * resp * 60 - mailcost
  scaled_profit
}
```

shuffle training data  

```{r}
set.seed(1234)
train <- train[sample(nrow(train)), ]
```  

## MODEL ONE - RANGER

### Create Grid Search Hyperparameter 

```{r}
# Establish a list of possible values for minsplit and maxdepth
mtry <- seq(3, 7, 2)
node_size <- seq(80, 120, 20)
sampe_size <- c(.55, .632, .70, .80)

# Create a data frame containing all combinations
RF_grid <- expand.grid(mtry = mtry, sample.fraction = sampe_size, min.node.size = node_size)

# Check out the grid
head(RF_grid)

# Print the number of grid combinations
nrow(RF_grid)
```  


### Train Ranger

```{r}
RF_models <- read_rds("Nawen/RF_models453.rds")
```

# Number of potential models in the grid
num_models <- nrow(RF_grid)

# Create an empty list to store models
RF_models <- list()

# Write a loop over the rows of hyper_grid to train the grid of models
for (i in 1:num_models) {

    mtry <- RF_grid$mtry[i]
    sample.fraction <- RF_grid$sample.fraction[i]
    min.node.size <- RF_grid$min.node.size[i]
    
    RF_models[[i]] <- ranger(res1 ~ .- numords_version1-last_version1 , probability = TRUE,
              data = train, num.trees = 501,
              mtry = mtry, respect.unordered.factors = TRUE, seed = 1234,
              sample.fraction = sample.fraction,
              min.node.size = min.node.size)
}



### Val Test to find the best model

```{r}
num_models <- length(RF_models)

profit_rf_values <- c()

for (i in 1:num_models) {
  mod <- RF_models[[i]]

  pred_val_RF <- predict(mod, val)

  # pred_rfr <- pred_rfr$predictions[, 1]

  profit_rf_values[i] <- perf_eval(tibble(
    res1 = val$res1,
    mailto_wave2 = pred_val_RF$predictions[, 1] > breakeven_response * 2
  ))
}

ind <- which.max(profit_rf_values)
best_RF <- RF_models[[ind]]
```


### Save predictions and profit
```{r}
### Predict with best model
pred_rgr <- predict(best_RF, val)
perf_eval(tibble(res1 = val$res1, mailto_wave2 = pred_rgr$predictions[, 1] > breakeven_response * 2))
```


### NN 5 fold CV  

We use five-fold grid-search to find the find model

```{r}
NN_models <- read_rds("NN_modcv.rds")
```


```{r}
nrow(train) / 5

## Create 5 folds
f1 <- seq(1, 10500)
f2 <- seq(10500 + 1, 10500 * 2)
f3 <- seq(10500 * 2 + 1, 10500 * 3)
f4 <- seq(10500 * 3 + 1, 10500 * 4)
f5 <- seq(10500 * 4 + 1, 10500 * 5)

## Create grid search
size <- seq(1, 5, 1)
decay <- c(0.1, 0.5, 0.1)
test <- list(f1, f2, f3, f4, f5)
nn_grid <- expand.grid(size = size, decay = decay, indx = test)

# Print the number of grid combinations
nrow(nn_grid)
```  


num_NN_model <- nrow(nn_grid)
NN_models <- list()
set.seed(1234)
for (i in 1:num_NN_model) {
  
  size <- nn_grid$size[i]
  decay <- nn_grid$decay[i]
  test_indx <- nn_grid$indx[[i]]
  
  NN_models[[i]] <- nn(
    train[-(test_indx), ], 
    rvar = "res1", 
    evar = c('zip_bins', 'numords', 'dollars', 'last', 'version1', 'owntaxprod', 'upgraded', 'zip801', 'zip804'),
    lev = "Yes", 
    size = size,
    decay = decay,
    seed = 1234 
    )
}  

saveRDS(NN_models, "NN_modcv.rds")


### Find best models
```{r}
num_models <- length(NN_models)

profit_nn_values <- c()

for (i in 1:num_models) {
  test_indx <- nn_grid$indx[[i]]
  mod <- NN_models[[i]]

  pred_tr_nn <- predict(mod, pred_data = train[test_indx, ])
  profit_nn_values[i] <- perf_eval(tibble(
    res1 = train[test_indx, ]$res1,
    mailto_wave2 = pred_tr_nn$Prediction > breakeven_response * 2
  ))
}

ind <- which.max(profit_nn_values)

best_NN <- NN_models[[6]]
```  


### Try on Val-set(smaller)
```{r}
pred_val_nn <- predict(best_NN, pred_data = val)
perf_eval(tibble(res1 = val$res1, mailto_wave2 = pred_val_nn$Prediction > breakeven_response * 2))
## 441046.2
```  


### NN with more Features  

```{r}
NN_models2 <- read_rds("NN2_modcv.rds")
```


```{r}
trainNN <- intuit %>%
  filter(training == 1) %>%
  select(res1, zip_bins, sex, bizflag, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804)
valNN <- intuit %>%
  filter(training == 0) %>%
  select(res1, zip_bins, sex, bizflag, numords, dollars, last, version1, owntaxprod, upgraded, zip801, zip804)
# str(train_dat)
nrow(trainNN) / 5

## Create 5 folds
f1 <- seq(1, 10500)
f2 <- seq(10500 + 1, 10500 * 2)
f3 <- seq(10500 * 2 + 1, 10500 * 3)
f4 <- seq(10500 * 3 + 1, 10500 * 4)
f5 <- seq(10500 * 4 + 1, 10500 * 5)

## Create grid search

size2 <- seq(2, 6, 1)
decay2 <- c(0.1, 0.5, 0.1)
test <- list(f1, f2, f3, f4, f5)

nn_grid2 <- expand.grid(size = size2, decay = decay2, indx = test)

# Print the number of grid combinations
nrow(nn_grid2)
```  


## train models
num_NN_model2 <- nrow(nn_grid2)
NN_models2 <- list()
set.seed(1234)
for (i in 1:num_NN_model2) {
  
  size2 <- nn_grid2$size[i]
  decay2 <- nn_grid2$decay[i]
  test_indx <- nn_grid2$indx[[i]]
  
  NN_models2[[i]] <- nn(
    trainNN[-(test_indx), ], 
    rvar = "res1", 
    evar = c('zip_bins',  'sex', 'bizflag', 'numords', 'dollars', 'last', 'version1', 'owntaxprod', 'upgraded', 'zip801', 'zip804'),
    lev = "Yes", 
    size = size2,
    decay = decay2,
    seed = 1234 
    )
  
}
saveRDS(NN_models2, "NN2_modcv.rds")



### Test to find best NN models
```{r}
num_models2 <- length(NN_models2)
profit_nn_values2 <- c()

for (i in 1:num_models2) {
  test_indx <- nn_grid2$indx[[i]]
  mod <- NN_models2[[i]]

  pred_tr_nn1 <- predict(mod, pred_data = trainNN[test_indx, ])

  profit_nn_values2[i] <- perf_eval(tibble(
    res1 = trainNN[test_indx, ]$res1,
    mailto_wave2 = pred_tr_nn1$Prediction > breakeven_response * 2
  ))
}

ind <- which.max(profit_nn_values2) ## 69 w/ more var

best_NN2 <- NN_models2[[ind]]
```  


### Predict using the best NN

```{r}
pred_val_nn1 <- predict(best_NN2, pred_data = valNN)
perf_eval(tibble(res1 = valNN$res1, mailto_wave2 = pred_val_nn1$Prediction > breakeven_response * 2))
```  



## XGBoosting


```{r}
# N <- nrow(train)*0.8
train_xgb <- train[, -c(11, 12)]
test_xgb <- val[, -c(11, 12)]

# using one hot encoding
labels <- ifelse(train_xgb$res1 == "Yes", 1, 0)
ts_label <- ifelse(test_xgb$res1 == "Yes", 1, 0)

new_tr <- model.matrix(~ . + 0, data = train_xgb[, -1])
new_ts <- model.matrix(~ . + 0, data = test_xgb[, -1])

# preparing matrix
dtrain <- xgb.DMatrix(data = new_tr, label = labels)
dtest <- xgb.DMatrix(data = new_ts, label = ts_label)
```  


### Tune hyperparameters:  

```{r}
set.seed(1234)
paramsX <- list(booster = "gbtree", objective = "binary:logistic", gamma = 10, subsample = 0.5, colsample_bytree = 0.8, max_delta_step = 10, min_child_weight = 10, lambda = 1.2)

xgbcv <- xgb.cv(params = paramsX, data = dtrain, nrounds = 100, nfold = 5, showsd = T, stratified = T, print.every.n = 10, early.stop.round = 50, maximize = F)
```  

We use the xgb.cv function to test different hyperparameters combination, looking for the train/test error and search for the best rounds which will be then used on training. Below is the tunning log. We used the log to set grid search parameters and eventually to find the best models.

paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=10, subsample=0.5, colsample_bytree=0.8, max_delta_step=0.5)
set.seed(1234)
xtr <- xgb.train (data = dtrain, nrounds = 27, params = paramsX)
450899.4


paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=9, subsample=0.5, colsample_bytree=0.8, max_delta_step=0.5)
xtr <- xgb.train (data = dtrain, nrounds = 39, params = paramsX)
453546.6


paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=9, subsample=0.5, colsample_bytree=0.8, max_delta_step=0.4)
xtr <- xgb.train (data = dtrain, nrounds = 65, params = paramsX)


paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=9, subsample=0.6, colsample_bytree=0.8, max_delta_step=0.5)
xtr <- xgb.train (data = dtrain, nrounds = 46, params = paramsX)
449149.8


paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=20, subsample=0.5, colsample_bytree=0.8, max_delta_step=0.5)
xtr <- xgb.train (data = dtrain, nrounds = 57, params = paramsX)
452743.6



paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=30, subsample=0.5, colsample_bytree=0.8, max_delta_step=10)
xtr <- xgb.train (data = dtrain, nrounds = 40, params = paramsX)
452829.1



paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=30, subsample=0.5, colsample_bytree=0.8, max_delta_step=10, min_child_weight=10)
xtr <- xgb.train (data = dtrain, nrounds = 40, params = paramsX)
454830



paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=9, subsample=0.5, colsample_bytree=0.8, max_delta_step=1, min_child_weight=5)
xtr <- xgb.train (data = dtrain, nrounds = 27, params = paramsX)
454524



paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=10, subsample=0.5, colsample_bytree=0.8, max_delta_step=10, min_child_weight=10)
xtr <- xgb.train (data = dtrain, nrounds = 38, params = paramsX)
463779.3
We increased max_delta_step +2, profit doesnâ€™t change



paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=11, subsample=0.5, colsample_bytree=0.8, max_delta_step=8, min_child_weight=8)
xtr <- xgb.train (data = dtrain, nrounds = 33, params = paramsX)
455749.1



set.seed(1234)
paramsX<- list(booster = "gbtree", objective = "binary:logistic", gamma=11, subsample=0.5, colsample_bytree=0.8, max_delta_step=11, min_child_weight=11)
xtr <- xgb.train (data = dtrain, nrounds = 28, params = paramsX)
458458.4



paramsX<- list(booster = "gbtree", objective = "binary:logistic", eta = 0.1, gamma=5, subsample=0.5, colsample_bytree=0.8, max_delta_step=10, min_child_weight=10)
xtr <- xgb.train (data = dtrain, nrounds = 14, params = paramsX)
46308.93



### Grid Search One:
```{r}
XGB_model1 <- read_rds("Nawen/XGB_model452.rds")
```


eta <- c(0.05, 0.1, 0.5, 1, 1.15)
max_depth <- seq(6, 22, 4)
subsample <- seq(0.2, 1, 0.2)
colsample_bytree <- seq(0.6, 1, 0.1)

XGB_grid <- expand.grid(eta = eta, max_depth = max_depth, subsample = subsample, colsample_bytree = colsample_bytree)

num_XGB_models <- nrow(XGB_grid)
XGB_models <- list()


for (i in 1:num_XGB_models){
  eta <- XGB_grid$eta[i]
  max_depth <- XGB_grid$max_depth[i]
  subsample <- XGB_grid$subsample[i]
  colsample_bytree <- XGB_grid$colsample_bytree[i]
  
  params <- list(booster = "gbtree", objective = "binary:logistic", eta=eta, gamma=0, max_depth=max_depth, min_child_weight=1, subsample=subsample, colsample_bytree=colsample_bytree, scale_pos_weight=20)
  
  XGB_models[[i]] <- xgb.train(params = params, data = dtrain, nrounds = 40)
  
}

### Test to find the best nodel
```{r}
num_model <- length(XGB_model1)
profit_xgb_values1 <- c()

for (i in 1:num_model) {
  mod <- XGB_model1[[i]]

  pred_val_xgb1 <- predict(mod, dtest)

  profit_xgb_values1[i] <- perf_eval(tibble(
    res1 = val$res1,
    mailto_wave2 = pred_val_xgb1 > breakeven_response * 2
  ))
}

ind <- which.max(profit_xgb_values1)
best_xgb1 <- XGB_model1[[ind]]
```


### Predict on Validation set
```{r}
xpred1 <- predict(best_xgb1, dtest)

perf_eval(tibble(res1 = val$res1, mailto_wave2 = xpred > breakeven_response * 2))
```


### Grid Search Two:
```{r}
XGB_models <- read_rds("XGB_model2.rds")
```


gamma <- c(8,9,10)
max_delta_step <- c(8,9,10)
min_child_weight <- c(8,9,10)
XGB_grid <- expand.grid(gamma=gamma, max_delta_step = max_delta_step, min_child_weight = min_child_weight)

num_XGB_models <- nrow(XGB_grid)
XGB_models <- list()

set.seed(1234)
for (i in 1:num_XGB_models){
  
  gamma <- XGB_grid$gamma[i]
  max_delta_step <- XGB_grid$max_delta_step[i]
  min_child_weight <- XGB_grid$min_child_weight[i]
  
  params <- list(booster = "gbtree", objective = "binary:logistic",gamma=gamma, subsample=0.5, colsample_bytree=0.8, max_delta_step=max_delta_step, min_child_weight=min_child_weight)
  
  XGB_models[[i]] <- xgb.train (params = params, data = dtrain, nrounds = 38)
}
saveRDS(XGB_models, "XGB_model2.rds")


### Test to find the best model
```{r}
num_models <- length(XGB_models)
profit_xgb_values <- c()

for (i in 1:num_models) {
  mod <- XGB_models[[i]]
  pred_val_xgb <- predict(mod, dtest)
  profit_xgb_values[i] <- perf_eval(tibble(
    res1 = val$res1,
    mailto_wave2 = pred_val_xgb > breakeven_response * 2
  ))
}

ind <- which.max(profit_xgb_values)
best_XGB <- XGB_models[[ind]]
```


### Test on valiadation set
```{r}
xpred <- predict(best_XGB, dtest)
perf_eval(tibble(res1 = val$res1, mailto_wave2 = xpred > breakeven_response * 2))
```


## Ensembel  
```{r}
val_pred <- cbind(rf_pred = pred_rgr$predictions[, 1], pred_nn = pred_val_nn$Prediction, pred_xgb = xpred, pred_xgb1 = xpred1, pred_xgb1 = xpred1)

mean_resp <- rowMeans(val_pred)
ensembel_profit <- perf_eval(tibble(res1 = val$res1, mailto_wave2 = mean_resp > breakeven_response * 2))
```  


### Select ID
```{r}
Nawen_Xinyue_Zichen_Zheng_Team12 <- data.frame(id = id, mailto_wave2 = mean_resp > breakeven_response * 2)

saveRDS(Nawen_Xinyue_Zichen_Zheng_Team12, "Nawen_Xinyue_Zichen_Zheng_Team12.rds")
```












